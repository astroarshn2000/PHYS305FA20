{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Announcements\n",
    "- __Please familiarize yourself with the term projects, and sign up for your (preliminary) choice__ using [this form](https://forms.gle/ByLLpsthrpjCcxG89). _You may revise your choice, but I'd recommend settling on a choice well before Thanksgiving._\n",
    "- Problem Set 5 posted on D2L, due Oct 20.\n",
    "- __Outlook__: algorithms for solving high-dimensional linear and non-linear equations; then Boundary Value Problems and Partial Differential Equations.\n",
    "- Conference for Undergraduate Women in Physics: online event in 2021, [applications accepted until 10/25](https://www.aps.org/programs/women/cuwip/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents as selection of topics from the book \"Numerical Linear Algebra\" by Trefethen and Bau (SIAM, 1997), and uses notebooks by Kyle Mandli."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditioning and Stability\n",
    "\n",
    "Once an approximation to a linear system is constructed the next question is how much trust can we put in the approximation? Since the true solution is not known, one of the few tools we have is to ask how well the approximation matches the original equation. In other words, we seek a solution to a system,\n",
    "$$\n",
    "    \\vec{f}(\\vec{x}) = \\vec{b}. \n",
    "$$\n",
    "\n",
    "We do not have $\\vec{x}$ but instead have an approximation, $\\hat{x}$, and we hope that \n",
    "$$\n",
    "    \\vec{f}(\\hat{x}) \\approx \\vec{b}.\n",
    "$$\n",
    "In this section the question we explore is to try to determine a bound on the relative error, $\\frac{||\\vec{x}-\\hat{x}||}{||\\vec{x}||}$ given the matrix, $A$. \n",
    "\n",
    "This leads to the notion of conditioning. Conditioning is the behavior of a problem when the solution is a changed a small bit (perturbed), and it is a  mathematical (analytic) property of the original system of equations.  Stability, on the other hand, is concerned with how the algorithm used to obtain an approximation behaves when the approximation is perturbed.\n",
    "\n",
    "## Conditioning and Condition Numbers\n",
    "\n",
    "A **well-conditioned** problem is one where a small perturbation to the original problem leads to only small changes in the solution.\n",
    "\n",
    "Formally we can think of a function $f$ which maps $x$ to $y$\n",
    "\n",
    "$$\n",
    "    f(x) = y \\quad \\text{or} \\quad f: X \\rightarrow Y.\n",
    "$$\n",
    "\n",
    "Let $x \\in X$ where we perturb $x$ with $\\delta x$ and we ask how the result $y$ changes:\n",
    "\n",
    "$$\n",
    "    ||f(x) - f(x + \\delta x)|| \\leq C ||x - (x+\\delta x)||\n",
    "$$\n",
    "\n",
    "for some constant $C$ possible dependent on $\\delta x$ depending on the type of conditioning we are considering.\n",
    "\n",
    "### Absolute Condition Number\n",
    "\n",
    "If we let $\\delta x$ be the small perturbation to the input and $\\delta f = f(x + \\delta x) - f(x)$ be the result the **absolute condition number** $\\hat{~\\kappa}$ can be defined as\n",
    "\n",
    "$$\n",
    "    \\hat{\\!\\kappa} = \\sup_{\\delta x} \\frac{||\\delta f||}{||\\delta x||}\n",
    "$$\n",
    "\n",
    "for most problems (assuming $\\delta f$ and $\\delta x$ are both infinitesimal).  \n",
    "\n",
    "When $f$ is differentiable we can evaluate the condition number via the Jacobian.  Recall that the derivative of a multi-valued function can be termed in the form of a Jacobian $J(x)$ where\n",
    "$$\n",
    "    [J(x)]_{ij} = \\frac{\\partial f_i}{\\partial x_j}(x).\n",
    "$$\n",
    "\n",
    "This allows us to write the infinitesimal $\\delta f$ as\n",
    "$$\n",
    "    \\delta f \\approx J(x) \\delta x\n",
    "$$\n",
    "with equality when $||\\delta x|| \\rightarrow 0$.  Then we can write the condition number as\n",
    "$$\n",
    "    \\hat{\\!\\kappa} = ||J(x)||\n",
    "$$\n",
    "where the norm is the one induced by the spaces $X$ and $Y$.\n",
    "\n",
    "### Relative Condition Number\n",
    "\n",
    "The **relative condition number** is defined similarly and is related to the difference before between the absolute error and relative error as defined previously.  With the same caveats as before it can be defined as\n",
    "$$\n",
    "    \\kappa = \\sup_{\\delta x} \\left( \\frac{\\frac{||\\delta f||}{||f(x)||}}{\\frac{||\\delta x||}{||x||}} \\right).\n",
    "$$\n",
    "\n",
    "Again if $f$ is differentiable we can use the Jacobian $J(x)$ to evaluate the relative condition number as\n",
    "$$\n",
    "    \\kappa = \\frac{||J(x)||}{||f(x)|| ~/ ~||x||}.\n",
    "$$\n",
    "\n",
    "#### Example\n",
    "Calculate the relative condition number of $\\sqrt{x}$ for $x > 0$. \n",
    "\n",
    "$$\n",
    "    f(x) = \\sqrt{x}, \\quad J(x) = f'(x) = \\frac{1}{2 \\sqrt{x}} \\\\\n",
    "    \\kappa = \\frac{||J(x)||}{||f(x)|| / ||x||} = \\frac{1}{2 \\sqrt{x}} \\frac{x}{\\sqrt{x}} = \\frac{1}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The condition number of a function was discussed in general terms above. Now, we examine the more specific case of a linear function, a matrix-vector multiplication. Here we let $\\vec{f}(\\vec{x})=Ax$ and determine the condition number by perturbing $x$.\n",
    "\n",
    "We begin with the definition above,\n",
    "$$\\begin{aligned}\n",
    "    \\kappa &= \\sup_{\\delta x} \\left ( \\frac{||A (\\vec{x}+\\delta x) - A \\vec{x}||}{||A\\vec{x}||} \\frac{||\\vec{x}||}{||\\delta x||}\\right ), \\\\\n",
    "    &= \\sup_{\\delta x} \\frac{ ||A \\delta x||}{||\\delta x||} \\frac{||\\vec{x}||}{||A\\vec{x}||}, \\\\\n",
    "    &= ||A|| \\frac{||\\vec{x}||}{||A \\vec{x}||},\n",
    "\\end{aligned}$$\n",
    "where $\\delta x$ is a vector.\n",
    "\n",
    "If $A$ has an inverse, then we note that\n",
    "$$\n",
    "\\begin{align}\n",
    " \\vec{x} &= A^{-1}A \\vec{x}, \\\\\n",
    " \\Rightarrow ||\\vec{x}|| &= || A^{-1}A \\vec{x} ||, \\\\\n",
    "                         &\\leq ||A^{-1}|| || A \\vec{x} ||,\n",
    "\\end{align}\n",
    "$$\n",
    "which implies that\n",
    "$$\n",
    "    \\frac{||x||}{||A x||} \\leq ||A^{-1}||.\n",
    "$$\n",
    "_We can now bound the condition number for a matrix by_\n",
    "$$\n",
    "    \\kappa \\leq ||A|| ||A^{-1}||.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condition Number of a Matrix\n",
    "\n",
    "The condition number of a matrix is defined by the product\n",
    "$$\n",
    "    \\kappa(A) = ||A||~||A^{-1}||.\n",
    "$$\n",
    "where here we are thinking about the matrix rather than a problem.  If $\\kappa$ is small than $A$ is said to be **well-conditioned**.  If $A$ is singular we assign $\\kappa(A) = \\infty$ as the matrix's condition number.\n",
    "\n",
    "When we are considering the $\\ell_2$ norm then we can write the condition number as\n",
    "\n",
    "$$\n",
    "    \\kappa(A) = \\frac{\\sqrt{\\rho(A^\\ast A)}}{\\sqrt{\\rho((A^\\ast A)^{-1})}} = \\frac{\\sqrt{\\max |\\lambda|}}{\\sqrt{\\min |\\lambda|}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condition Number of a System of Equations\n",
    "\n",
    "Another way to think about the conditioning of a problem we have looked at before is that the matrix $A$ itself is an input to the problem.  Consider than the system of equations $A\\vec{x} = \\vec{b}$ where we will perturb both $A$ and $\\vec{x}$ resulting in\n",
    "$$\n",
    "    (A + \\delta A)(\\vec{x} + \\delta x) = \\vec{b}.\n",
    "$$\n",
    "\n",
    "Assuming we solve the problem exactly we know that $A\\vec{x} = \\vec{b}$ and that the infinitesimals multiplied $\\delta A \\delta x$ are smaller than the other term, and the above expression can be approximation by\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    (A + \\delta A)(\\vec{x} + \\delta x) &= \\vec{b}, \\\\\n",
    "    A\\vec{x} + \\delta Ax + A \\delta x + \\delta A \\delta \\vec{x} &= \\vec{b} \\\\\n",
    "    \\delta A\\vec{x} + A \\delta x & = 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Solving for $\\delta x$ leads to \n",
    "$$\n",
    "    \\delta x = -A^{-1} \\delta A \\vec{x}\n",
    "$$\n",
    "implying\n",
    "$$\n",
    "    ||\\delta x|| \\leq ||A^{-1}|| ~ ||\\delta A|| ~ ||\\vec{x}||\n",
    "$$\n",
    "and therefore\n",
    "$$\n",
    "    \\frac{\\frac{||\\delta x||}{||\\vec{x}||}}{\\frac{||\\delta A||}{||A||}} \\leq ||A^{-1}||~||A|| = \\kappa(A).\n",
    "$$\n",
    "\n",
    "We can also say the following regarding the condition number of a system of equations then\n",
    "\n",
    "**Theorem:**  Let $\\vec{b}$ be fixed and consider the problem of computing $\\vec{x}$ in $A\\vec{x} = \\vec{b}$ where $A$ is square and non-singular.  The condition number of this problem with respect to perturbations in $A$ is the condition number of the matrix $\\kappa(A)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stability\n",
    "\n",
    "We now return to the consideration of the fact that we are interested not only in the well-conditioning of a mathematical problem but in how we might solve it on a finite precision machine.  In some sense conditioning describes how well we can solve a problem in exact arithmetic and stability how well we can solve the problem in finite arithmetic.  \n",
    "\n",
    "### Accuracy and Stability\n",
    "\n",
    "As we have defined before we will consider **absolute error** as\n",
    "$$\n",
    "    ||F(x) - f(x)||\n",
    "$$\n",
    "where $F(x)$ is the approximation to the true solution $f(x)$.  Similarly we can define **relative error** as\n",
    "$$\n",
    "    \\frac{||F(x) - f(x)||}{||f(x)||}.\n",
    "$$\n",
    "In the ideal case we would like the relative error to be $\\mathcal{O}(\\epsilon_{\\text{machine}})$.\n",
    "\n",
    "#### Forwards Stability\n",
    "\n",
    "A **forward stable** algorithm for $x \\in X$ has\n",
    "\n",
    "$$\n",
    "    \\frac{||F(x) - f(x)||}{||f(x)||} = \\mathcal{O}(\\epsilon_{\\text{machine}})\n",
    "$$\n",
    "\n",
    "In other words\n",
    "> A forward stable algorithm gives almost the right answer to exactly the right question.\n",
    "\n",
    "#### Backwards Stability\n",
    "\n",
    "A stronger notion of stability can also be defined which is satisfied by many approaches in numerical linear algebra.  We say that an algorithm $F$ is **backward stable** if for $x \\in X$ we have\n",
    "\n",
    "$$\n",
    "    F(x) = f(\\hat{\\!x})\n",
    "$$\n",
    "\n",
    "for some $\\hat{\\!x}$ with\n",
    "\n",
    "$$\n",
    "    \\frac{||\\hat{\\!x} - x||}{||x||} = \\mathcal{O}(\\epsilon_{\\text{machine}}).\n",
    "$$\n",
    "\n",
    "In other words\n",
    "> A backward stable algorithm gives exactly the right answer to nearly the right question.\n",
    "\n",
    "Combining these ideas along with the idea that we should not expect to be able to accurately compute the solution to a poorly conditioned problem we can form the mixed forward-backward sense of stability as for $x \\in X$ if\n",
    "\n",
    "$$\n",
    "    \\frac{||F(x) - f(\\hat{\\!x})||}{||f(\\hat{\\!x})||} = \\mathcal{O}(\\epsilon_{\\text{machine}})\n",
    "$$\n",
    "\n",
    "for some $\\hat{\\!x}$ with \n",
    "\n",
    "$$\n",
    "    \\frac{||\\hat{\\!x} - x||}{||x||} = \\mathcal{O}(\\epsilon_{\\text{machine}}).\n",
    "$$\n",
    "\n",
    "In other words\n",
    "> A stable algorithm gives nearly the right answer to nearly the right question.\n",
    "\n",
    "_An important aspect of the above statement is that we can not necessarily guarantee an accurate result.  If the condition number $\\kappa(x)$ is small we would expect that a stable algorithm would give us an accurate result (by definition).  This is reflected in the following theorem._\n",
    "\n",
    "**Theorem:**  Suppose a backward stable algorithm is applied to solve a problem $f: X \\rightarrow Y$ with condition number $\\kappa$ on a finite precision machine, then the relative errors satisfy\n",
    "$$\n",
    "    \\frac{||F(x) - f(\\hat{\\!x})||}{||f(\\hat{\\!x})||} = \\mathcal{O}(\\kappa(x) ~ \\epsilon_{\\text{machine}}).\n",
    "$$\n",
    "\n",
    "**Proof:**  By the definition of the condition number of a problem we can write\n",
    "$$\n",
    "    \\frac{||F(x) - f(\\hat{\\!x})||}{||f(\\hat{\\!x})||} \\leq (\\kappa(x) + \\mathcal{O}(\\epsilon_{\\text{machine}}))\\frac{||\\hat{\\!x} - x||}{||x||}.\n",
    "$$\n",
    "Combining this with the definition of backwards stability we can arrive at the statement of the theorem.\n",
    "\n",
    "To summarize:\n",
    "> **Backward Error Analysis** - Process of using the condition number of the problem and stability of the algorithm to determine the error.\n",
    "\n",
    "> **Forward Error Analysis** - Considers the accrual of error at each step of an algorithm given slightly perturbed input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigenproblems\n",
    "\n",
    "## Overview\n",
    "\n",
    "We will now consider eigenproblems of the form\n",
    "$$\n",
    "    A x = \\lambda x\n",
    "$$\n",
    "where $A \\in \\mathbb C^{m \\times m}$, $x \\in \\mathbb C^m$ and $\\lambda \\in \\mathbb C$.  The vector $x$ is known as the **eigenvector** and $\\lambda$ the **eigenvalue**.  The set of all eigenvalues is called the **spectrum** of $A$.\n",
    "\n",
    "### Eigenvalue Decomposition\n",
    "Similar to QR factorization, an eigendecomposition is possible such that $A$ can be written as\n",
    "\n",
    "$$\n",
    "    A = X \\Lambda X^{-1}\n",
    "$$\n",
    "\n",
    "where $X$ is the matrix formed by the eigenvectors $x$ as its columns and $\\Lambda$ is a diagonal matrix with the eigenvalues along its diagonal.  \n",
    "\n",
    "This equation comes from the similar equation $A X = X \\Lambda$ which is of course related to the original problem statement.  This latter equation can be written out as\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix} \n",
    "          &   &   &   &   \\\\\n",
    "          &   &   &   &   \\\\\n",
    "          &   & A &   &   \\\\\n",
    "          &   &   &   &   \\\\\n",
    "          &   &   &   &  \n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix} \n",
    "          &   &   &   &   \\\\\n",
    "          &   &   &   &   \\\\\n",
    "        x_1 & x_2 & \\cdots & x_{m-1} & x_m \\\\\n",
    "          &   &   &   &   \\\\\n",
    "          &   &   &   &  \n",
    "    \\end{bmatrix} = \n",
    "    \\begin{bmatrix} \n",
    "          &   &   &   &   \\\\\n",
    "          &   &   &   &   \\\\\n",
    "        x_1 & x_2 & \\cdots & x_{m-1} & x_m \\\\\n",
    "          &   &   &   &   \\\\\n",
    "          &   &   &   &  \n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix} \n",
    "        \\lambda_1 &   &   &   &   \\\\\n",
    "          & \\lambda_2 &   &   &   \\\\\n",
    "          &   & \\ddots &   &   \\\\\n",
    "          &   &   & \\lambda_{m-1} &   \\\\\n",
    "          &   &   &   & \\lambda_m\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Here we note that the eigenpair $(x_j, \\lambda_j)$ are matched as the $j$th column of $X$ and the $j$th element of $\\Lambda$ on the diagonal. \n",
    "\n",
    "**Algebraic multiplicity** is the number of times overall an eigenvalue repeats itself.\n",
    "\n",
    "**Geometric multiplicity** is defined as the number of linearly independent eigenvectors that belong to each eigenvalue.\n",
    "\n",
    "If the algebraic multiplicity is equal to the geometric multiplicity for all $\\lambda$ then we can say that there is a full eigenspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example:  Computing Multiplicities\n",
    "\n",
    "Compute the geometric and algebraic multiplicities for the following matrices.  What is the relationship between the algebraic and geometric multiplicities?\n",
    "\n",
    "$$A = \\begin{bmatrix} \n",
    "    2 &   &  \\\\\n",
    "      & 2 &  \\\\\n",
    "      &   & 2 \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$B = \\begin{bmatrix} 2\n",
    "      & 1 &   \\\\\n",
    "      & 2 & 1 \\\\\n",
    "      &   & 2 \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "1. The characteristic polynomial of $A$ is\n",
    "  \n",
    "  $$\n",
    "      \\mathcal{P}_A(z) = (2 - z)(2 - z)(2 - z) = (2 - z)^3\n",
    "  $$\n",
    "  \n",
    "  so the eigenvalues are all $\\lambda = 2$ so we know the algebraic multiplicity is 3 of this eigenvalue.  The geometric multiplicity is determined by the number of linearly independent eigenvectors.  For this matrix we have three eigenvectors that are all linearly independent which happen to be the unit vectors in each direction (check!).  This means that the geometric multiplicity is also 3.\n",
    "\n",
    "1. The characteristic polynomial of $B$ is the same as $A$ so again we know $\\lambda = 2$ but now we need to be a bit careful about the eigenvectors.  In this case the only eigenvector is a scalar multiple of $e_1$ so the geometric multiplicity is 1.\n",
    "\n",
    "### Interpretations of the Eigenspace\n",
    "\n",
    "One way to interpret the eigenproblem is that of one that tries to find the subspaces of $\\mathbb C^m$ which act like scalar multiplication by $\\lambda$.  The eigenvectors associated with one eigenvalue then form a subspace of $S \\subseteq \\mathbb C^m$.\n",
    "\n",
    "When an eigenvalue has algebraic multiplicity that equals its geometric then it is called non-defective and otherwise defective.  This property is also inherited to the matrix so in the above example $A$ and $B$ are non-defective and defective matrices respectively.\n",
    "\n",
    "### Determinant and Trace\n",
    "\n",
    "Two important properties of matrices have important relationships with their eigenvalues, namely the determinant and trace.  The determinant we have seen, the **trace** is defined as the sum of the elements on the diagonal of a matrix, in other words\n",
    "$$\n",
    "    \\text{tr}(A) = \\sum^m_{i=1} A_{ii}.\n",
    "$$\n",
    "\n",
    "The relationship between the determinant and the eigenvalues is not difficult to guess due to the nature of the characteristic polynomial.  The trace of a diagonal matrix is clear and provides another suggestion to the relationship.\n",
    "\n",
    "**Theorem:** The determinant $\\det(A)$ and trace $\\text{trace}(A)$ are equal to the product and sum of the eigenvalues of $A$ respectively counting algebraic multiplicity.\n",
    "\n",
    "### Similarity Transformations\n",
    "\n",
    "The relationship between a matrix's eigenvalues and its determinant and trace are due to the special relationship between the eigenvalue decomposition and what is called similarity transformations.  A **similarity transformation** is defined as a transformation that takes A and maps it to $X^{-1} A X$ (assuming $X$ is non-singular).  Two matrices are said to be **similar** if there is a similarity transformation between them.  \n",
    "\n",
    "The most important property of similar matrices is that they have the same characteristic polynomial, eigenvalues, and multiplicities.\n",
    "\n",
    "This allows us to relate geometric and algebraic multiplicity as \n",
    "\n",
    "**Theorem:** The algebraic multiplicity of an eigenvalue $\\lambda$ is at least as great as its geometric multiplicity.\n",
    "\n",
    "### Schur Factorization\n",
    "\n",
    "A **Schur factorization** of a matrix $A$ is defined as\n",
    "\n",
    "$$\n",
    "    A = Q T Q^\\ast\n",
    "$$\n",
    "\n",
    "where $Q$ is unitary and $T$ is upper-triangular.  In particular note that due do the structure of the resulting characteristic polynomial that $A$ and $T$ have identical eigenvalues. \n",
    "\n",
    "**Theorem:** Every matrix $A \\in \\mathbb C^{m \\times m}$ has a Schur factorization.\n",
    "\n",
    "Note that the above results imply the following\n",
    " - An eigen-decomposition $A = X \\Lambda X^{-1}$ exists if and only if $A$ is non-defective (it has a complete set of eigenvectors)\n",
    " - A unitary transformation $A = Q \\Lambda Q^\\ast$ exists if and only if $A$ is normal ($A^\\ast A = A A^\\ast$)\n",
    " - A Schur factorization always exists\n",
    " \n",
    "Note that each of these lead to a means for isolating the eigenvalues of a matrix and will be useful when considering algorithms for finding them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condition Number of a Simple Eigenvalue\n",
    "\n",
    "Before we discuss a number of approaches to computing eigenvalues it good to consider what the condition number of a given eigenproblem is.  \n",
    "\n",
    "Let \n",
    "$$\n",
    "    Ax = \\lambda x\n",
    "$$ \n",
    "define the eigenvalue problem in question.  Here we will introduce a related problem \n",
    "$$\n",
    "    y^\\ast A = \\lambda y^\\ast\n",
    "$$ \n",
    "where $y$ is the **left eigenvector** and from before $x$ is the **right eigenvector**.  These vectors also can be shown to have the relationship $y^\\ast x \\neq 0$ for a simple eigenvalue.\n",
    "\n",
    "Now consider the perturbed problem\n",
    "$$\n",
    "    (A + \\delta A) (x + \\delta x) = (\\lambda + \\delta \\lambda) (x + \\delta x).\n",
    "$$\n",
    "Expanding this and throwing out quadratic terms and removing the eigenproblem we have\n",
    "$$\n",
    "    \\delta A x + A \\delta x = \\delta \\lambda x + \\lambda \\delta x.\n",
    "$$\n",
    "\n",
    "Multiple both sides of the above by the left eigenvector and use $y^\\ast x \\neq 0$ to find\n",
    "$$\\begin{aligned}\n",
    "    y^\\ast \\delta A x + y^\\ast A \\delta x &= y^\\ast \\delta \\lambda x + y^\\ast \\lambda \\delta x \\\\\n",
    "    y^\\ast \\delta A x &= y^\\ast \\delta \\lambda x\n",
    "\\end{aligned}$$\n",
    "where we again use the slightly different definition of the eigenproblem.  We can then solve for $\\delta \\lambda$ to find\n",
    "$$\n",
    "    \\delta \\lambda = \\frac{y^\\ast \\delta A x}{y^\\ast x}\n",
    "$$\n",
    "meaning that the ratio between the dot-product of the left and right eigenvectors and the conjugate dot-product of the matrix $\\delta A$ then form a form of bound on the expected error in the simple eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Eigenvalues\n",
    "\n",
    "The most obvious approach to computing eigenvalues is a direct computation of the roots of the characteristic polynomial.  Unfortunately the following theorem suggests this is not a good way to compute eigenvalues:\n",
    "\n",
    "**Theorem:** For an $m \\geq 5$ there is a polynomial $\\mathcal{P}(z)$ of degree $m$ with rational coefficients that has a real root $\\mathcal{P}(z_0) = 0$ with the property that $z_0$ cannot be written using any expression involving rational numbers, addition, subtraction, multiplication, division, and $k$th roots.\n",
    "\n",
    "Not all is lost however, we just cannot use any direct methods to solve for the eigenvalues.  Instead we must use an iterative approach, in other words we want to construct a sequence that converges to the eigenvalues.  How does this relate to how we found roots previously?\n",
    "\n",
    "Almost all approaches to computing eigenvalues do so through the computation of the Schur factorization.  The Schur factorization, as we have seen, will preserve the eigenvalues.  The steps to compute the Schur factorization are usually broken down into two steps\n",
    "1. Directly transform $A$ into a **Hessenberg** matrix, a matrix that contains zeros below its first sub-diagonal, directly using Householder reflections.\n",
    "1. Use an iterative method to change the sub-diagonal into all zeros\n",
    "\n",
    "### Hessenberg and Tridiagonal form\n",
    "\n",
    "What we want to do is construct a sequence of unitary matrices that turns $A$ into a Hessenberg matrix to start.  We can use Householder reflections to do this with the important distinction that we only want to remove zeros below the first sub-diagonal.  The sequence would look something like\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        \\text{x} & \\text{x} & \\text{x} & \\text{x} & \\text{x} \\\\\n",
    "        \\text{x} & \\text{x} & \\text{x} & \\text{x} & \\text{x} \\\\\n",
    "        \\text{x} & \\text{x} & \\text{x} & \\text{x} & \\text{x} \\\\\n",
    "        \\text{x} & \\text{x} & \\text{x} & \\text{x} & \\text{x} \\\\\n",
    "        \\text{x} & \\text{x} & \\text{x} & \\text{x} & \\text{x}\n",
    "    \\end{bmatrix} \\overset{Q_1}{\\rightarrow}\n",
    "    \\begin{bmatrix}\n",
    "        \\text{x} & \\text{x} & \\text{x}& \\text{x} & \\text{x} \\\\\n",
    "        \\text{x} & \\text{x} & \\text{x}& \\text{x} & \\text{x} \\\\\n",
    "        0 & \\text{x} & \\text{x}& \\text{x} & \\text{x} \\\\\n",
    "        0 & \\text{x} & \\text{x}& \\text{x} & \\text{x} \\\\\n",
    "        0 & \\text{x} & \\text{x}& \\text{x} & \\text{x}\n",
    "    \\end{bmatrix} \\overset{Q_2}{\\rightarrow}\n",
    "    \\begin{bmatrix}\n",
    "        \\text{x} & \\text{x} & \\text{x}& \\text{x} & \\text{x} \\\\\n",
    "        \\text{x} & \\text{x} & \\text{x}& \\text{x} & \\text{x} \\\\\n",
    "        0 & \\text{x} & \\text{x}& \\text{x} & \\text{x} \\\\\n",
    "        0 & 0 & \\text{x}& \\text{x} & \\text{x} \\\\\n",
    "        0 & 0 & \\text{x}& \\text{x} & \\text{x} \n",
    "    \\end{bmatrix} \\overset{Q_3}{\\rightarrow}\n",
    "    \\begin{bmatrix}\n",
    "        \\text{x} & \\text{x} & \\text{x}& \\text{x} & \\text{x} \\\\\n",
    "        \\text{x} & \\text{x} & \\text{x}& \\text{x} & \\text{x} \\\\\n",
    "        0 & \\text{x} & \\text{x}& \\text{x} & \\text{x} \\\\\n",
    "        0 & 0 & \\text{x}& \\text{x} & \\text{x} \\\\\n",
    "        0 & 0 & 0 & \\text{x} & \\text{x}\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "so we have the sequence $Q^\\ast_1 A Q_1$.  Note we need both to preserve the entries of the first column that are not being transformed to zeros.\n",
    "\n",
    "One important special case of this sequence of transformations is that if the matrix $A$ is hermitian (the matrix is its own conjugate transpose, $A = A^\\ast$, or symmetric in the real case) then the Hessenberg matrix is tridiagonal.\n",
    "\n",
    "We now will focus on how to formulate the iteration step of the eigenproblem.  We will also restrict our attention to symmetric, real matrices.  This implies that all eigenvalues will be real and have a complete set of orthogonal eigenvectors.  Generalizations can be made of many of the following algorithms but is beyond the scope of this class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rayleigh Quotient and Inverse Iteration\n",
    "\n",
    "There are a number of classical approaches to computing the iterative step above which we will review here.  Inverse power iteration in particular is today still the dominant means of finding the eigenvectors once the eigenvalues are known.\n",
    "\n",
    "### Rayleigh Quotient\n",
    "\n",
    "The **Rayleigh quotient** of a vector $x \\in \\mathbb R^m$ is the scalar\n",
    "$$\n",
    "    r(x) = \\frac{x^T A x}{x^T x}.\n",
    "$$\n",
    "The importance of the Rayleigh quotient is made clear when we evaluate $r(x)$ at an eigenvector.  When this is the case the quotient evaluates to the corresponding eigenvalue. \n",
    "\n",
    "The Rayleigh quotient can be motivated by asking the question, given an eigenvector $x$, what value $\\alpha$ acts most like an eigenvalue in an $\\ell_2$ sense:\n",
    "$$\n",
    "    \\min_\\alpha ||A x - \\alpha x||_2.\n",
    "$$\n",
    "\n",
    "This can be reformulated as a least-squares problem noting that $x$ is the \"matrix\", $\\alpha$ is the unknown vector (scalar) and $Ax$ is the right-hand side so we have\n",
    "$$\n",
    "    (x^T x) \\alpha = x^T (A x)\n",
    "$$\n",
    "which can be solved so that\n",
    "$$\n",
    "    \\alpha = r(x) = \\frac{x^T A x}{x^T x}.\n",
    "$$\n",
    "\n",
    "### Power Iteration\n",
    "\n",
    "Power iteration is a straight forward approach to finding the eigenvector of the largest eigenvalue of $A$.  The basic idea is that the sequence\n",
    "$$\n",
    "    \\frac{x}{||x||}, \\frac{Ax}{||Ax||}, \\frac{A^2x}{||A^2x||}, \\frac{A^3x}{||A^3x||}, \\ldots\n",
    "$$\n",
    "will converge (although very slowly) to the desired eigenvector.\n",
    "\n",
    "We implement this method by initializing the algorithm with some vector $v$ with $||v|| = 1$.  We then apply the sequence of multiplications.\n",
    "\n",
    "The reason why this works can be seen by considering the initial vector $v$ as a linear combination of the orthonormal eigenvectors (which we have assumed exist) such that\n",
    "\n",
    "$$\n",
    "    v^{(0)} = a_1 q_1 + a_2 q_2 + \\cdots + a_m q_m.\n",
    "$$\n",
    "\n",
    "Multiplying $v^{(0)}$ by $A$ then leads to\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    Av^{(0)} = v^{(1)} &= a_1 A q_1 + a_2 A q_2 + \\cdots + a_m A q_m \\\\\n",
    "    &= c_1 (a_1 \\lambda_1 q_1 + a_2 \\lambda_2 q_2 + \\cdots + a_m \\lambda_m q_m) \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "where $c_1$ is some constant due to the fact the eigenvectors are not uniquely specified.  Repeating this $k$ times we have\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    Av^{(k-1)} = v^{(k)} &= a_1 A^k q_1 + a_2 A^k q_2 + \\cdots + a_m A^k q_m \\\\\n",
    "    &= c_k (a_1 \\lambda_1^k q_1 + a_2 \\lambda_2^k q_2 + \\cdots + a_m \\lambda_m^k q_m) \\\\\n",
    "    &= c_k \\lambda_1^k \\left(a_1 q_1 + a_2 \\frac{\\lambda_2^k}{\\lambda_1^k} q_2 + \\cdots + a_m \\frac{\\lambda_m^k}{\\lambda_1^k} q_m \\right)\n",
    "\\end{aligned}$$\n",
    "\n",
    "Since $\\lambda_1 > \\lambda_i$ for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Iteration\n",
    "\n",
    "Inverse iteration uses a similar approach with the difference being that we can use it to find any of the eigenvectors for the matrix $A$.  \n",
    "\n",
    "Consider the matrix \n",
    "\n",
    "$$\n",
    "    (A - \\mu I)^{-1},\n",
    "$$ \n",
    "\n",
    "the eigenvectors of this matrix are the same as $A$ with the eigenvalues \n",
    "\n",
    "$$\n",
    "    (\\lambda_j - \\mu)^{-1}\n",
    "$$ \n",
    "\n",
    "where $\\lambda_j$ are the eigenvalues of $A$.  \n",
    "\n",
    "If $\\mu$ is close to a particular $\\lambda_j$, say $\\lambda_J$, then \n",
    "\n",
    "$$\n",
    "    (\\lambda_J - \\mu)^{-1}\n",
    "$$ \n",
    "\n",
    "will be larger than any of the other $(\\lambda_j - \\mu)^{-1}$.  In this way we effectively have picked out the eigenvalue we want to consider in the power iteration!\n",
    "\n",
    "### Rayleigh Quotient Iteration\n",
    "\n",
    "By themselves the above approaches are not particularly useful but combining them we can iterate back and forth to find the eigenvalue, eigenvector pair:\n",
    "1. Compute the Rayleigh quotient and find an estimate for $\\lambda_j$\n",
    "1. Compute one step of inverse iteration to approximate $x_j$\n",
    "1. Repeat..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "m = 3\n",
    "A = numpy.array([[2, 1, 1], [1, 3, 1], [1, 1, 4]])\n",
    "\n",
    "num_steps = 10\n",
    "v = numpy.empty((num_steps, m))\n",
    "lam = numpy.empty(num_steps)\n",
    "\n",
    "v[0, :] = numpy.array([1, 1, 1])\n",
    "v[0, :] = v[0, :] / numpy.linalg.norm(v[0, :], ord=2)\n",
    "lam[0] = numpy.dot(v[0,:], numpy.dot(A, v[0, :]))\n",
    "for k in range(1, num_steps):\n",
    "    w = numpy.linalg.solve(A - lam[k-1] * numpy.identity(m), v[k-1, :])\n",
    "    v[k, :] = w / numpy.linalg.norm(w, ord=2)\n",
    "    lam[k] = numpy.dot(v[k,:], numpy.dot(A, v[k, :]))\n",
    "    \n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.semilogy(range(10), numpy.abs(lam - numpy.linalg.eigvals(A)[0]), 'o')\n",
    "\n",
    "axes.set_title(\"Convergence of eigenvalue\")\n",
    "axes.set_xlabel(\"Step\")\n",
    "axes.set_ylabel(\"Error\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QR Algorithm\n",
    "\n",
    "The most basic use of a $QR$ factorization to find eigenvalues is to iteratively compute the factorization and multiply the resulting $Q$ and $R$ in the reverse order.  This sequence will eventually converge to the Schur decomposition of the matrix $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A(0) =\n",
      "[[ 4.166667e+00  1.095445e+00 -1.267105e+00]\n",
      " [ 1.095445e+00  2.000000e+00  3.187966e-17]\n",
      " [-1.267105e+00  2.153667e-16  2.833333e+00]]\n",
      "\n",
      "A(1) =\n",
      "[[ 5.090909  0.15743   0.623249]\n",
      " [ 0.15743   1.86182  -0.54704 ]\n",
      " [ 0.623249 -0.54704   2.047271]]\n",
      "\n",
      "A(2) =\n",
      "[[ 5.198682 -0.075903 -0.207275]\n",
      " [-0.075903  2.181835  0.496554]\n",
      " [-0.207275  0.496554  1.619484]]\n",
      "\n",
      "A(3) =\n",
      "[[ 5.211648 -0.065892  0.058223]\n",
      " [-0.065892  2.363696 -0.321368]\n",
      " [ 0.058223 -0.321368  1.424655]]\n",
      "\n",
      "A(4) =\n",
      "[[ 5.213778 -0.035826 -0.01528 ]\n",
      " [-0.035826  2.430805  0.183737]\n",
      " [-0.01528   0.183737  1.355417]]\n",
      "\n",
      "A(5) =\n",
      "[[ 5.214202e+00 -1.758371e-02  3.920120e-03]\n",
      " [-1.758371e-02  2.451920e+00 -1.007513e-01]\n",
      " [ 3.920120e-03 -1.007513e-01  1.333877e+00]]\n",
      "\n",
      "A(6) =\n",
      "[[ 5.214294e+00 -8.392060e-03 -9.988671e-04]\n",
      " [-8.392060e-03  2.458212e+00  5.453885e-02]\n",
      " [-9.988671e-04  5.453885e-02  1.327494e+00]]\n",
      "\n",
      "A(7) =\n",
      "[[ 5.214314e+00 -3.973357e-03  2.540044e-04]\n",
      " [-3.973357e-03  2.460055e+00 -2.940989e-02]\n",
      " [ 2.540044e-04 -2.940989e-02  1.325631e+00]]\n",
      "\n",
      "A(8) =\n",
      "[[ 5.214318e+00 -1.876920e-03 -6.455358e-05]\n",
      " [-1.876920e-03  2.460591e+00  1.584128e-02]\n",
      " [-6.455358e-05  1.584128e-02  1.325090e+00]]\n",
      "\n",
      "A(9) =\n",
      "[[ 5.214319e+00 -8.860220e-04  1.640309e-05]\n",
      " [-8.860220e-04  2.460747e+00 -8.529903e-03]\n",
      " [ 1.640309e-05 -8.529903e-03  1.324933e+00]]\n",
      "\n",
      "True eigenvalues: \n",
      "[5.21432  2.460811 1.324869]\n",
      "\n",
      "Computed eigenvalues: \n",
      "5.214319458186374\n",
      "2.4607473568633944\n",
      "1.324933184950233\n"
     ]
    }
   ],
   "source": [
    "%precision 6\n",
    "m = 3\n",
    "A = numpy.array([[2, 1, 1], [1, 3, 1], [1, 1, 4]])\n",
    "MAX_STEPS = 10\n",
    "\n",
    "for i in range(MAX_STEPS):\n",
    "    Q, R = numpy.linalg.qr(A)\n",
    "    A = numpy.dot(R, Q)\n",
    "    print()\n",
    "    print(\"A(%s) =\" % (i))\n",
    "    print(A)\n",
    "\n",
    "print()\n",
    "print(\"True eigenvalues: \")\n",
    "print(numpy.linalg.eigvals(A))\n",
    "print()\n",
    "print(\"Computed eigenvalues: \")\n",
    "for i in range(m):\n",
    "    print(A[i, i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So why does this work?  The first step is to find the $QR$ factorization of $A^{(k-1)}$ which is equivalent to finding\n",
    "\n",
    "$$\n",
    "    (Q^{(k)})^T A^{(k-1)} = R^{(k)}\n",
    "$$\n",
    "\n",
    "and multiplying on the right leads to\n",
    "\n",
    "$$\n",
    "    (Q^{(k)})^T A^{(k-1)} Q^{(k)} = R^{(k)} Q^{(k)}.\n",
    "$$\n",
    "\n",
    "In this way we can see that this is a similarity transformation of the matrix $A^{(k-1)}$ since the $Q^{(k)}$ is an orthogonal matrix ($Q^{-1} = Q^T$). This of course is not a great idea to do directly but works great in this case as we iterate to find the upper triangular matrix $R^{(k)}$ which is exactly where the eigenvalues appear.\n",
    "\n",
    "In practice this basic algorithm is modified to include a few additions:\n",
    "\n",
    "1. Before starting the iteration $A$ is reduced to tridiagonal form.\n",
    "1. Motivated by the inverse power iteration we observed we instead consider a shifted matrix $A^{(k)} - \\mu^{(k)} I$ for factoring.  The $\\mu$ picked is related to the estimate given by the Rayleigh quotient.  Here we have\n",
    "\n",
    "$$\n",
    "    \\mu^{(k)} = \\frac{(q_m^{(k)})^T A q_m^{(k)}}{(q_m^{(k)})^T q_m^{(k)}} = (q_m^{(k)})^T A q_m^{(k)}.\n",
    "$$\n",
    "\n",
    "1. Deflation is used to reduce the matrix $A^{(k)}$ into smaller matrices once (or when we are close to) finding an eigenvalue to simplify the problem.\n",
    "\n",
    "This has been the standard approach until recently for finding eigenvalues of a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatives\n",
    "\n",
    "### Jacobi\n",
    "\n",
    "Jacobi iteration employs the idea that we know the eigenvalues of a matrix of size equal to or less than 4 (we know the roots of the characteristic polynomial directly).  Jacobi iteration therefore attempts to break the matrix down into at most 4 by 4 matrices along the diagonal via a series of similarity transformations based on only diagonalizing sub-matrices 4 by 4 or smaller.\n",
    "\n",
    "### Bisection \n",
    "\n",
    "It turns out if you do not want all of the eigenvalues of a matrix that using a bisection method to find some subset of the eigenvalues is often the most efficient way to get these.  This avoids the pitfall of trying to find the eigenvalues via other root-finding approaches by only needing evaluations of the function and if a suitable initial guess is provided can find the eigenvalue quickly that is closest to the initial bracket provided.\n",
    "\n",
    "### Divide-and-conquer\n",
    "\n",
    "This algorithm is actually the one used most often used if both eigenvalues and eigenvectors are needed and performs up to twice as fast as the $QR$ approach.  The basic idea is to split the matrix into two pieces at every iteration by introducing zeros on the appropriate off-diagonals which neatly divides the problem into two pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
