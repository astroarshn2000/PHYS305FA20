{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Announcements\n",
    "- __Please familiarize yourself with the term projects, and sign up for your (preliminary) choice__ using [this form](https://forms.gle/ByLLpsthrpjCcxG89). _You may revise your choice, but I'd recommend settling on a choice well before Thanksgiving._\n",
    "- Problem Set 5 will be posted on D2L on Oct 12 (i.e., later today), due Oct 20.\n",
    "- __Outlook__: algorithms for solving high-dimensional linear and non-linear equations; then Boundary Value Problems and Partial Differential Equations.\n",
    "- Conference for Undergraduate Women in Physics: online event in 2021, [applications accepted until 10/25](https://www.aps.org/programs/women/cuwip/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QR Factorizations\n",
    "\n",
    "## Projections\n",
    "\n",
    "A **projector** is a square matrix $P$ that satisfies\n",
    "$$\n",
    "    P^2 = P.\n",
    "$$\n",
    "\n",
    "A projector comes from the idea that we want to project a vector $v$ onto a lower dimensional subspace.  For example, suppose that $v$ lies completely within the subspace, i.e. $\\vec{v} \\in \\text{range}(P)$. If that is the case then $P \\vec{v}$ should not change, or $P\\vec{v} = \\vec{v}$.  This motivates the definition above.\n",
    "\n",
    "If $P\\vec{v} = \\vec{v}'$ then $\\vec{v}'$ should be in the sub-space and subsequent applications of $P$ should result in no change, i.e.\n",
    "$$\n",
    "    P( P \\vec{v} ) = P\\vec{v}' = \\vec{v}'.\n",
    "$$\n",
    "\n",
    "As another example, take a vector $\\vec{x} \\notin \\text{range}(P)$ and project it onto the subspace $P\\vec{x} = \\vec{v}$.  If we apply the projection again to $\\vec{v}$ we now have\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    P\\vec{x} &= \\vec{v}, \\\\\n",
    "    P^2 \\vec{x} & = P \\vec{v} = \\vec{v} \\\\\n",
    "    \\Rightarrow P^2 &= P.\n",
    "\\end{aligned}$$\n",
    "\n",
    "It is also important to keep in mind the following, given again $\\vec{x} \\notin \\text{range}(P)$, if we look at the difference between the projection and the original vector $P\\vec{x} - \\vec{x}$ and apply the projection again we have\n",
    "$$\n",
    "    P(P\\vec{x} - \\vec{x}) = P^2 \\vec{x} - P\\vec{x} = 0\n",
    "$$\n",
    "which means the difference between the projected vector $P\\vec{x} = \\vec{v}$ lies in the null space of $P$, $\\vec{v} \\in \\text{null}(P)$.\n",
    "\n",
    "### Complementary Projectors\n",
    "\n",
    "A projector also has a complement defined to be $I - P$, which is also a projector.\n",
    "\n",
    "A projector and its complement divide a space into two subspaces whose intersection is \n",
    "$$\n",
    "    \\text{range}(I - P) \\cap \\text{range}(P) = \\{0\\}\n",
    "$$\n",
    "or\n",
    "$$\n",
    "    \\text{null}(P) \\cap \\text{range}(P) = \\{0\\}\n",
    "$$\n",
    "These two spaces are called **complementary subspaces**.\n",
    "\n",
    "Given this property we can take any $P \\in \\mathbb C^{m \\times m}$ which will split $\\mathbb C^{m \\times m}$ into two subspaces $S$ and $V$, assume that $\\vec{s}\\in S = \\text{range}(P)$, and $\\vec{v} \\in V = \\text{null}(P)$.  If we have $\\vec{x} \\in \\mathbb C^{m \\times m}$ that we can split the vector $\\vec{x}$ into components in $S$ and $V$ by using the projections\n",
    "$$\\begin{aligned}\n",
    "    P \\vec{x} = \\vec{x}_S& &\\vec{x}_s \\in S \\\\\n",
    "    (I - P) \\vec{x} = \\vec{x}_V& &\\vec{x}_V \\in V\n",
    "\\end{aligned}$$\n",
    "which we can also observe adds to the original vector as\n",
    "$$\n",
    "    \\vec{x}_S + \\vec{x}_V = P \\vec{x} + (I - P) \\vec{x} = \\vec{x}.\n",
    "$$\n",
    "\n",
    "Try constructing a projection matrix so that $P \\in \\mathbb R^3$ that projects a vector into one of the coordinate directions ($\\mathbb R$).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthogonal Projectors\n",
    "\n",
    "An **orthogonal projector** is one that projects onto a subspace $S$ that is orthogonal to the complementary subspace $V$ (this is also phrased that $S$ projects along a space $V$).  Note that we are only talking about the subspaces (and their basis), not the projectors!\n",
    "\n",
    "A **hermitian** matrix is one whose complex conjugate is itself, i.e.\n",
    "$$\n",
    "    P = P^\\ast.\n",
    "$$\n",
    "\n",
    "With this definition we can then say:  *A projector $P$ is orthogonal if and only if $P$ is hermitian.*\n",
    "\n",
    "### Projection with an Orthonormal Basis\n",
    "\n",
    "We can also directly construct a projector that uses an orthonormal basis on the subspace $S$.  If we define another matrix $Q \\in \\mathbb C^{m \\times n}$ which is unitary (its columns are orthonormal) we can construct an orthogonal projector as\n",
    "$$\n",
    "    P = Q Q^*.\n",
    "$$\n",
    "Note that the resulting matrix $P$ is in $\\mathbb C^{m \\times m}$ as we require.  This means also that the dimension of the subspace $S$ is $n$.\n",
    "\n",
    "#### Example:  Construction of an orthonormal projector\n",
    "\n",
    "Take $\\mathbb R^3$ and derive a projector that projects onto the x-y plane and is an orthogonal projector.\n",
    "\n",
    "$$\n",
    "    Q = \\begin{bmatrix} 1 & 0 \\\\ \n",
    "                        0 & 1 \\\\ \n",
    "                        0 & 0 \n",
    "        \\end{bmatrix}, \\quad \n",
    "    Q Q^\\ast = \\begin{bmatrix} 1 & 0 \\\\ \n",
    "                               0 & 1 \\\\ \n",
    "                               0 & 0 \n",
    "               \\end{bmatrix} \n",
    "               \\begin{bmatrix} 1 & 0 & 0 \\\\\n",
    "                               0 & 1 & 0 \n",
    "               \\end{bmatrix} = \\begin{bmatrix} \n",
    "                               1 & 0 & 0 \\\\\n",
    "                               0 & 1 & 0 \\\\ \n",
    "                               0 & 0 & 0 \n",
    "               \\end{bmatrix}\n",
    "$$\n",
    "#### Example: Construction of a projector that eliminates a direction\n",
    "\n",
    "Goal:  Eliminate the component of a vector in the direction $\\vec{q}$.\n",
    "\n",
    "Form the projector $P = \\vec{q} \\vec{q}^\\ast \\in \\mathbb C^{m \\times m}$.  The complement $I - P$ will then include everything **BUT** that direction.  If $||\\vec{q}|| = 1$ we can then simply use $I - \\vec{q} \\vec{q}^\\ast$.  If not we can write the projector in terms of the arbitrary vector $\\vec{a}$ as\n",
    "$$\n",
    "    I - \\frac{\\vec{a} \\vec{a}^\\ast}{||\\vec{a}||^2} = I - \\frac{\\vec{a} \\vec{a}^\\ast}{\\vec{a}^\\ast \\vec{a}}\n",
    "$$\n",
    "Note that differences in the resulting dimensions between the two values in the fraction.  Also note that as we saw with the outer product, the resulting $\\text{rank}(\\vec{a} \\vec{a}^\\ast) = 1$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "q = numpy.array([0, 0, 1])\n",
    "P = numpy.outer(q, q.conjugate())\n",
    "P_comp = numpy.identity(3) - P\n",
    "\n",
    "x = numpy.array([3, 4, 5])\n",
    "print(numpy.dot(P, x))\n",
    "print(numpy.dot(P_comp, x))\n",
    "\n",
    "#now repeat to show that the normalization is correct\n",
    "a = numpy.array([0, 0, 3])\n",
    "P = numpy.outer(a, a.conjugate()) / (numpy.dot(a, a.conjugate()))\n",
    "P_comp = numpy.identity(3) - P\n",
    "print(numpy.dot(P, x))\n",
    "print(numpy.dot(P_comp, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QR Factorization\n",
    "\n",
    "One of the most important ideas in linear algebra is the concept of factorizing an original matrix into different constituents that may have useful properties.  These properties can help us understand the matrix better and lead to numerical methods.  In numerical linear algebra one of the most important factorizations is the **QR factorization**.  \n",
    "\n",
    "The basic idea is that we want to break up $A$ into its successive spaces spanned by the columns of $A$.  If we have\n",
    "$$\n",
    "    A = \\begin{bmatrix}  &  &  \\\\  &  &  \\\\ \\vec{a}_1 & \\cdots & \\vec{a}_n \\\\  &  &  \\\\  &  &  \\end{bmatrix}\n",
    "$$\n",
    "where the columns of $A$ are linearly independent then we want to construct the sequence\n",
    "$$\n",
    "    \\text{span}(\\vec{a}_1) \\subseteq \\text{span}(\\vec{a}_1, \\vec{a}_2) \\subseteq \\text{span}(\\vec{a}_1, \\vec{a}_2, \\vec{a}_3) \\subseteq \\cdots \\subseteq \\text{span}(\\vec{a}_1, \\vec{a}_2, \\ldots , \\vec{a}_n)\n",
    "$$\n",
    "where here $\\text{span}(\\vec{v}_1,\\vec{v}_2,\\ldots,\\vec{v}_m)$ indicates the subspace spanned by the vectors $\\vec{v}_1, \\vec{v}_2, \\ldots, \\vec{v}_m$.  \n",
    "\n",
    "QR factorization attempts to construct a set of orthonormal vectors $\\vec{q}_i$ that span each of the subspaces, i.e. \n",
    "$$\n",
    "    \\text{span}(\\vec{a}_1, \\vec{a}_2, \\ldots, \\vec{a}_j) = \\text{span}(\\vec{q}_1, \\vec{q}_2, \\ldots, \\vec{q}_j).\n",
    "$$\n",
    "\n",
    "Before specifying a general procedure we go through the first few steps of the process to get a feel for what needs to be done. In particular we are given a sequence of vectors, $\\vec{a_1}$, $\\vec{a_2}$, $\\ldots$, $\\vec{a_n}$, and generate a sequence of orthonormal vectors  $\\vec{q}_1$, $\\vec{q}_2$, $\\ldots$, and $\\vec{q}_n$ whose spans are the same subspace.\n",
    "\n",
    "1. For $\\text{span}(\\vec{a}_1)$ we can directly use $\\vec{a}_1$ but normalize the vector such that\n",
    "$$\n",
    "    \\vec{q}_1 = \\frac{\\vec{a}_1}{||\\vec{a}_1||}.\n",
    "$$\n",
    "\n",
    "1. For $\\text{span}(\\vec{a}_1, \\vec{a}_2)$ we already have $\\vec{q}_1$ so we need to have a vector $\\vec{q}_2$ that is orthogonal to $\\vec{q}_1$, i.e.\n",
    "$$\n",
    "    \\langle \\vec{q}_1, \\vec{q}_2 \\rangle = \\vec{q}_1^\\ast \\vec{q}_2 = 0\n",
    "$$\n",
    "and is again normalized.  We can accomplish this by modifying $\\vec{a}_2$ such that\n",
    "$$\n",
    "    \\vec{q}_2 = \\frac{\\vec{a}_2 - \\langle \\vec{q}_1, \\vec{a}_2\\rangle \\vec{q}_1}{||\\vec{a}_2 - \\langle \\vec{q}_1, \\vec{a}_2\\rangle\\vec{q}_1||}.\n",
    "$$\n",
    "which we can show is orthogonal to $\\vec{q}_1$ as\n",
    "$$\\begin{aligned}\n",
    "    \\langle \\vec{q}_1, \\vec{q}_2 \\rangle &= \\left(\\langle  \\vec{q}_1, \\vec{a}_2 - \\langle  \\vec{q}_1, \\vec{a}_2 \\rangle\\vec{q}_1 \\rangle \\right) \\frac{1}{||\\vec{a}_2 - \\langle  \\vec{q}_1, \\vec{a}_2 \\rangle \\vec{q}_1||} \\\\\n",
    "    &= \\left(\\langle  \\vec{q}_1, \\vec{a}_2 \\rangle - \\langle  \\vec{q}_1, \\vec{a}_2 \\rangle\\langle\\vec{q}_1,\\vec{q}_1\\rangle\\right) \\frac{1}{||\\vec{a}_2 - \\langle  \\vec{q}_1, \\vec{a}_2 \\rangle \\vec{q}_1||} \\\\\n",
    "    &= \\left(\\langle  \\vec{q}_1, \\vec{a}_2 \\rangle - \\langle  \\vec{q}_1, \\vec{a}_2 \\rangle\\right) \\frac{1}{||\\vec{a}_2 - \\langle  \\vec{q}_1, \\vec{a}_2 \\rangle \\vec{q}_1||} \\\\\n",
    "    &= 0.\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results suggest then that we may have a matrix factorization that has the following form:\n",
    "$$\n",
    "    \\begin{bmatrix}  &  &  \\\\  &  &  \\\\ \\vec{a}_1 & \\cdots & \\vec{a}_n \\\\  &  &  \\\\  &  &  \\end{bmatrix} = \n",
    "    \\begin{bmatrix}  &  &  \\\\  &  &  \\\\ \\vec{q}_1 & \\cdots & \\vec{q}_n \\\\  &  &  \\\\  &  &  \\end{bmatrix}\n",
    "    \\begin{bmatrix} r_{11} & r_{12} & \\cdots & r_{1n} \\\\  & r_{22} &  &  \\\\  &  & \\ddots & \\vdots \\\\  &  &  & r_{nn} \\end{bmatrix}.\n",
    "$$\n",
    "If we write this out as a matrix multiplication we have\n",
    "$$\\begin{aligned}\n",
    "    \\vec{a}_1 &= r_{11} \\vec{q}_1 \\\\\n",
    "    \\vec{a}_2 &= r_{22} \\vec{q}_2 + r_{12} \\vec{q}_1 \\\\\n",
    "    \\vec{a}_3 &= r_{33} \\vec{q}_3 + r_{23} \\vec{q}_2 + r_{13} \\vec{q}_1 \\\\\n",
    "    &\\vdots\n",
    "\\end{aligned}$$\n",
    "We can also identify at least the first couple of values of $\\vec{r}$ as\n",
    "$$\\begin{aligned}\n",
    "    r_{11} &= \\left \\Vert \\vec{a}_1 \\right \\Vert \\\\\n",
    "    r_{12} &= \\langle \\vec{q}_1, \\vec{a}_2 \\rangle \\\\\n",
    "    r_{22} &= \\left \\Vert \\vec{a}_2 - r_{12} \\vec{q}_1 \\right \\Vert\n",
    "\\end{aligned}$$\n",
    "It turns out we can generalize this as Gram-Schmidt orthogonalization.\n",
    "\n",
    "### Gram-Schmidt Orthogonalization\n",
    "\n",
    "As may have been suggestive we can directly construct the arrays $Q$ and $R$ via a process of successive orthogonalization.  We have already shown the first two iterations so lets now consider the $j$th iteration.  \n",
    "\n",
    "We want to subtract off the components of the vector $\\vec{a}_j$ in the direction of the $\\vec{q}_i$ vectors where $i=1,\\ldots,j-1$.  This suggests that we define a vector $\\vec{v}_j$ such that\n",
    "$$\\begin{aligned}\n",
    "    \\vec{v}_j &= \\vec{a}_j - \\langle \\vec{q}_1, \\vec{a}_j \\rangle \\vec{q}_1 - \\langle \\vec{q}_2, \\vec{a}_j \\rangle \\vec{q}_2 - \\quad \\cdots \\quad- \\langle \\vec{q}_{j-1}, \\vec{a}_j \\rangle \\vec{q}_{i-1} \\\\\n",
    "    &= \\vec{a}_j - \\sum^{j-1}_{i=1} \\langle \\vec{q}_i, \\vec{a}_j \\rangle \\vec{q}_i.\n",
    "\\end{aligned}$$\n",
    "\n",
    "We also need to normalize $\\vec{v}_j$ which allows to define the $j$th column of $Q$ as\n",
    "$$\n",
    "    \\vec{q}_j = \\frac{\\vec{a}_j - \\sum^{j-1}_{i=1} \\langle \\vec{q}_i, \\vec{a}_j \\rangle \\vec{q}_i}{\\left \\Vert \\vec{a}_j - \\sum^{j-1}_{i=1} \\langle \\vec{q}_i, \\vec{a}_j \\rangle \\vec{q}_i \\right \\Vert}.\n",
    "$$\n",
    "\n",
    "We can also discern what the entries of $R$ are as we can write the matrix multiplication as the sequence\n",
    "$$\\begin{aligned}\n",
    "    \\vec{q}_1 &= \\frac{\\vec{a}_1}{r_{11}} \\\\\n",
    "    \\vec{q}_2 &= \\frac{\\vec{a}_2 - r_{12} \\vec{q}_1}{r_{22}} \\\\\n",
    "    \\vec{q}_3 &= \\frac{\\vec{a}_3 - r_{13} \\vec{q}_1 - r_{23} \\vec{q}_2}{r_{33}} \\\\\n",
    "    &\\vdots \\\\\n",
    "    \\vec{q}_n &= \\frac{\\vec{a}_n - \\sum^{n-1}_{i=1} r_{in} \\vec{q}_i}{r_{nn}}\n",
    "\\end{aligned}$$\n",
    "leading us to define\n",
    "$$\n",
    "    r_{ij} = \\left \\{ \\begin{aligned}\n",
    "        &\\langle \\vec{q}_i, \\vec{a}_j \\rangle & &i \\neq j \\\\\n",
    "        &\\left \\Vert \\vec{a}_j - \\sum^{j-1}_{i=1} r_{ij} \\vec{q}_i \\right \\Vert & &i = j\n",
    "    \\end{aligned} \\right .\n",
    "$$\n",
    "\n",
    "This is called the **classical Gram-Schmidt** iteration.  Turns out that the procedure above is unstable because of rounding errors introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Classical Gram-Schmidt Iteration\n",
    "def classic_GS(A):\n",
    "    m = A.shape[0]\n",
    "    n = A.shape[1]\n",
    "    Q = numpy.empty((m, n))\n",
    "    R = numpy.zeros((n, n))\n",
    "    for j in range(n):\n",
    "        v = A[:, j]\n",
    "        for i in range(j):\n",
    "            R[i, j] = numpy.dot(Q[:, i].conjugate(), A[:, j])\n",
    "            v = v - R[i, j] * Q[:, i]\n",
    "        R[j, j] = numpy.linalg.norm(v, ord=2)\n",
    "        Q[:, j] = v / R[j, j]\n",
    "    return Q, R\n",
    "\n",
    "A = numpy.array([[12, -51, 4], [6, 167, -68], [-4, 24, -41]], dtype=float)\n",
    "Q, R = classic_GS(A)\n",
    "print(A)\n",
    "print(Q)\n",
    "print(numpy.dot(Q.transpose(), Q))\n",
    "print(R)\n",
    "print(numpy.dot(Q, R) - A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full vs. Reduced QR\n",
    "\n",
    "If the original matrix $A \\in \\mathbb C^{m \\times n}$ where $m \\ge n$ then we can still define a QR factorization, called the **full QR factorization**, which appends columns full of zeros to $R$ to reproduce the full matrix.\n",
    "$$\n",
    "    A = Q R = \\begin{bmatrix} Q_1 & Q_2 \\end{bmatrix} \n",
    "              \\begin{bmatrix} R_1 \\\\\n",
    "                              0 \n",
    "              \\end{bmatrix} = Q_1 R_1\n",
    "$$\n",
    "The factorization $Q_1 R_1$ is called the **reduced** or **thin QR factorization** of $A$.\n",
    "\n",
    "We require that the additional columns added $Q_2$ are an orthonormal basis that is orthogonal itself to $\\text{range}(A)$.  If $A$ is full ranked then $Q_1$ and $Q_2$ provide a basis for $\\text{range}(A)$ and $\\text{null}(A^\\ast)$ respectively.\n",
    "\n",
    "#### QR Existence and Uniqueness\n",
    "Two important theorems exist regarding this algorithm which we state without proof:\n",
    "\n",
    "*Every $A \\in \\mathbb C^{m \\times n}$ with $m \\geq n$ has a full QR factorization and therefore a reduced QR factorization.*\n",
    "\n",
    "*Each $A \\in \\mathbb C^{m \\times n}$ with $m \\geq n$ of full rank has a unique reduced QR factorization $A = QR$ with $r_{jj} > 0$.*\n",
    "\n",
    "#### Gram-Schmidt in Terms of Projections\n",
    "\n",
    "To start lets rewrite classical Gram-Schmidt as a series of projections:\n",
    "$$\n",
    "    \\vec{q}_1 = \\frac{P_1 \\vec{a}_1}{||P_1 \\vec{a}_1||}, \\quad \\vec{q}_2 = \\frac{P_2 \\vec{a}_2}{||P_2 \\vec{a}_2||}, \\quad \\cdots \\quad \\vec{q}_n = \\frac{P_n \\vec{a}_n}{||P_n \\vec{a}_n||}\n",
    "$$\n",
    "where the $P_i$ are orthogonal projectors onto the $\\vec{q}_1, \\vec{q}_2, \\ldots, \\vec{q}_{i-1}$, in other words the complement of $\\text{span}(\\vec{a}_1, \\vec{a}_2, \\ldots, \\vec{a}_{i-1})$.\n",
    "\n",
    "_How should we construct these projectors?_\n",
    "\n",
    "We saw before that we can easily construct an orthogonal projector onto the complement of a space by first constructing the projector onto the space itself via\n",
    "$$\n",
    "    \\hat{\\!Q}_{i-1} = \\begin{bmatrix}\n",
    "           &    &       &        \\\\\n",
    "           &    &       &        \\\\\n",
    "        \\vec{q}_1 & \\vec{q}_2 & \\cdots & \\vec{q}_{i-1} \\\\ \n",
    "           &    &       &        \\\\\n",
    "           &    &       &       \n",
    "    \\end{bmatrix}.\n",
    "$$\n",
    "Constructing the projection onto the space spanned by $\\vec{q}_1$ through $\\vec{q}_{i-1}$ is then\n",
    "$$\n",
    "    \\hat{\\!P}_{i-1} = \\hat{\\!Q}_{i-1} \\hat{\\!Q}^\\ast_{i-1}\n",
    "$$\n",
    "and therefore the projections in Gram-Schmidt orthogonalization is\n",
    "$$\n",
    "    P_{i} = I - \\hat{\\!P}_{i-1} = I - \\hat{\\!Q}_{i-1} \\hat{\\!Q}^\\ast_{i-1}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modified Gram-Schmidt\n",
    "\n",
    "One problem with the original Gram-Schmidt algorithm is it is not stable numerically.  Instead we can derive a modified method that is more numerically stable.\n",
    "\n",
    "Recall that the basic piece of the original algorithm was to take the inner product of $\\vec{a}_j$ and all the relevant $\\vec{q}_i$.  Using the rewritten version of Gram-Schmidt in terms of projections we then have\n",
    "\n",
    "$$\n",
    "    \\vec{v}_i = P_i \\vec{a}_i.\n",
    "$$\n",
    "\n",
    "This projection is of rank $m - (i - 1)$ as we know that the resulting $\\vec{v}_i$ are linearly independent by construction.  The modified version of Gram-Schmidt instead uses projections that are all of rank $m-1$.  To construct this projection remember that we can again construct the complement to a projection and perform the following sequence of projections\n",
    "\n",
    "$$\n",
    "    P_i = \\hat{\\!P}_{\\vec{q}_{i-1}} \\hat{\\!P}_{\\vec{q}_{i-2}} \\cdots \\hat{\\!P}_{\\vec{q}_{2}} \\hat{\\!P}_{\\vec{q}_{1}}\n",
    "$$\n",
    "\n",
    "where $\\hat{\\!P}_{\\vec{q}_{i}}$ projects onto the complement of the space spanned by $\\vec{q}_i$.  Note that this performs mathematically the same job as $P_i \\vec{a}_i$ however each of these projectors are of rank $m - 1$.  \n",
    "\n",
    "This leads to the following set of calculations:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    1.\\quad \\vec{v}^{(1)}_i &= \\vec{a}_i  \\\\\n",
    "    2.\\quad \\vec{v}^{(2)}_i &= \\hat{\\!P}_{\\vec{q}_1} \\vec{v}_i^{(1)} = \\vec{v}^{(1)}_i - \\vec{q}_1 q_1^\\ast \\vec{v}^{(1)}_i \\\\\n",
    "    3.\\quad \\vec{v}^{(3)}_i &= \\hat{\\!P}_{\\vec{q}_2} \\vec{v}_i^{(2)} = \\vec{v}^{(2)}_i - \\vec{q}_2 \\vec{q}_2^\\ast \\vec{v}^{(2)}_i \\\\\n",
    "    & \\text{  } \\vdots & &\\\\\n",
    "    i.\\quad \\vec{v}^{(i)}_i &= \\hat{\\!P}_{\\vec{q}_{i-1}} \\vec{v}_i^{(i-1)} =  \\vec{v}_i^{(i-1)} - \\vec{q}_{i-1} \\vec{q}_{i-1}^\\ast \\vec{v}^{(i-1)}_i\n",
    "\\end{aligned}$$\n",
    "\n",
    "The reason why this approach is more stable is that we are not projecting with a possibly arbitrarily low-rank projector, instead we only take projectors that are high-rank.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
