{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Announcements\n",
    "- Homework 6 will be posted today, covers finite difference methods + probability\n",
    "- Outlook: statistical inference (this + next week), then intro to maschine learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on course material developed together with Phil Marshall and Adam Mantz.\n",
    "\n",
    "## Goal for the next two weeks\n",
    "\n",
    "In a nutshell, this module is about *how data are turned into conclusions*.\n",
    "\n",
    "Most examples and problems are taken from astrophysics, but otherwise the content is extremely general.\n",
    "\n",
    "The scientific process:\n",
    "* Propose observations\n",
    "* Collect and \"reduce\" data\n",
    "* **Explore and summarize the data**\n",
    "* **Hypothesize and test**\n",
    "* **Interpret, conclude**, speculate\n",
    "* Report\n",
    "\n",
    "\"Turning data into conclusions\" broadly refers to the **bold** items.\n",
    "\n",
    "## Some key ideas\n",
    "\n",
    "* i) **All data we collect include some degree of randomness**\n",
    "* ii) **Any conclusions we draw must therefore incorporate uncertainty**\n",
    "This means we should describe both the data and conclusions in the language of mathematical probability.\n",
    "\n",
    "Our *conclusion* will take the form: the probability that something is true in light of (given) the data we collected.\n",
    "$p(\\mathrm{thing}|\\mathrm{data})$\n",
    "\n",
    "By the basic laws of probability, this can be written\n",
    "$p(\\mathrm{thing}|\\mathrm{data}) = \\frac{p(\\mathrm{data}|\\mathrm{thing}) p(\\mathrm{thing})}{p(\\mathrm{data})}$\n",
    "\n",
    "We'll unpack this much more later, but importantly it means that\n",
    "* iii) **There is a correct answer**\n",
    "Just like in physics, the theory tells us the solution. The challenge is in *evaluating it*.\n",
    "\n",
    "Within this framework,\n",
    "\n",
    "* iv) **Data are constants**\n",
    "Even though they are generated randomly by the Universe, *data that we have already collected are fixed numbers*.\n",
    "\n",
    "Much of our job boils down to building a model that predicts (probabilistically) what data we might have gotten.\n",
    "\n",
    "* v) **Things we don't know with perfect precision can be mathematically described as \"random\"**\n",
    "That is, we use probabilities to model things that are uncertain, even if they are not truly random.\n",
    "\n",
    "Again,\n",
    "* There is a correct answer\n",
    "* Unknowns, including *potential* data and our conclusions, are (mathematically) random\n",
    "* *Collected* data are constants\n",
    "\n",
    "# Review of Probability\n",
    "\n",
    "Goals:\n",
    "* Review the bits of mathematical probability that will be most important for us later.\n",
    "* Prime our brains for probabilistic reasoning.\n",
    "\n",
    "This will be quick -- the key concepts will already be familiar from other courses.\n",
    "\n",
    "### Some terminology\n",
    "* Sample space ($\\Omega$): the set of all possible answers/outcomes for a given question/experiment.\n",
    "* Event ($E$): any subset of $\\Omega$.\n",
    "\n",
    "The probability of an event will be a real function satisfying certain requirements...\n",
    "\n",
    "### Axioms of probability:\n",
    "* $\\forall E: 0 \\leq P(E) \\leq 1$\n",
    "* $P(\\Omega) = P\\left(\\bigcup_{\\mathrm{all~}i} E_i\\right) = 1$\n",
    "* If $E_i$ are mutually exclusive, $P\\left(\\bigcup_i E_i\\right) = \\sum_i P(E_i)$\n",
    "\n",
    "This dry definition provides a function with the right properties to describe our intuitive understanding of probability.\n",
    "\n",
    "### A familiar example\n",
    "\n",
    "Let $\\Omega$ be the set of states available to a system of fixed energy, e.g. a box full of gas particles.\n",
    "\n",
    "With one additional (physics) assumption, that it's equally probable for the system to occupy any state in $\\Omega$, this is the microcanonical ensemble in statistical mechanics.\n",
    "\n",
    "### Discrete vs. continuous sample spaces\n",
    "Very often the type of event we're interested in lives in a continuous sample space.\n",
    "\n",
    "Our axioms mostly translate straightforwardly; in this example $P(\\Omega)=1$ becomes the normalization condition\n",
    "\n",
    "$\\int_{-\\infty}^{\\infty} p(h=x)dx$ = 1\n",
    "\n",
    "We can always describe the discrete case as a continuous one where $p$ is a sum of Dirac delta functions.\n",
    "\n",
    "### More definitions\n",
    "If $X$ takes real values, then $p(X=x)$ is a **probability density** function, or PDF.\n",
    "* $p(X=x)$ is *not* a probability! But integrals like $p(X=x)dx$ and $P(x_0 < X < x_1)$ are.\n",
    "* We will rapidly become lazy and denote $p(X=x)$ incorrectly as $p(X)$ or $p(x)$. You have been warned.\n",
    "\n",
    "The first bullet is highly relevant if we ever want to change variables, e.g. $x\\rightarrow y(x)$\n",
    "* $p(y) \\neq p[x(y)]$; rather $p(y) = p(x) \\left|dx/dy\\right|$\n",
    "\n",
    "The **cumulative distribution function** (CDF) is the probability that $X\\leq x$.\n",
    "* Usually written: $F(x) = P(X \\leq x) = \\int_{-\\infty}^x p(X=x')dx'$.\n",
    "* Conversely, the PDF is the derivative of the CDF.\n",
    "* (The CDF is sometimes referred to just as the distribution function.)\n",
    "\n",
    "### Ridiculous example: an unfair coin toss\n",
    "We flip a coin which is weighted to land on heads a fraction $q$ of the time. To make things numeric, let $X=0$ for stand for an outcome of tails and $X=1$ for heads.\n",
    "\n",
    "X | PDF $p(X)$ &nbsp; &nbsp; | CDF $F(X)$ &nbsp; &nbsp; \n",
    ":---: | :------------: | :------------:\n",
    "0 |  $1-q$    |  $1-q$\n",
    "1 | $q$ |  1\n",
    "\n",
    "### Joint probability distributions\n",
    "Things get more interesting when we deal with joint distributions of multiple events, $p(X=x$ and $Y=y)$, or just $p(x,y)$.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"graphics/prob_joint_correlated.png\" width=75%> (Usually visualized as contours of <i>p</i>) </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "The **marginal probability** of $y$, $p(y)$, means the probability of $y$ *irrespective* of what $x$ is.\n",
    "* $p(y) = \\int dx ~ p(x,y)$\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"graphics/prob_joint_correlated.png\" width=75%></td>\n",
    "        <td><img src=\"graphics/prob_joint_marginal.png\" width=75%></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "The **conditional probability** of $y$ *given* a value of $x$, $p(y|x)$, is most easily understood this way\n",
    "* $p(x,y) = p(y|x)\\,p(x)$\n",
    "\n",
    "i.e., $p$ of getting $x$ AND $y$ can be *factorized* into the product of\n",
    "* $p$ of getting $x$ regardless of $y$, *and*\n",
    "* $p$ of getting $y$ given $x$.\n",
    "\n",
    "$p(y|x)$ is a (normalized) slice through $p(x,y)$ rather than an integral.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"graphics/prob_joint_correlated.png\" width=75%></td>\n",
    "        <td><img src=\"graphics/prob_joint_conditional.png\" width=75%></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "$x$ and $y$ are **independent** if $p(y|x) = p(y)$.\n",
    "\n",
    "Equivalently, $p(x,y) = p(x)\\,p(y)$.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"graphics/prob_joint_independent.png\" width=75%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Take the coin tossing example from earlier, where $P(\\mathrm{heads})=q$ and $P(\\mathrm{tails})=1-q$ for a given toss. Assume that this holds independently for each toss.\n",
    "\n",
    "Find:\n",
    "\n",
    "1. The conditional probability that both tosses are heads, given that the first toss is heads.\n",
    "2. The conditional probability that both tosses are heads, given that at least one of the tosses is heads.\n",
    "\n",
    "## Exercise\n",
    "Say we keep on tossing this coin, still assuming independence, a total of $N$ times. Work out the probability that exactly $n$ of these turn out to be heads.\n",
    "\n",
    "### How to count things\n",
    "The answer to the previous exercise is the PDF of the binomial distribution\n",
    "\n",
    "$P(n|q,N) = {N \\choose n} q^n (1-q)^{N-n}$\n",
    "\n",
    "To introduce some notation, we might write this as\n",
    "\n",
    "$n \\sim \\mathrm{Binom}(q,N)$\n",
    "\n",
    "Here the squiggle means \"is a random variable that is distributed as\" (as opposed to \"has the same order of magnitude as\" or \"scales with\", the common usages in physics).\n",
    "\n",
    "Recall that a key assumption was that each toss (trial) was independent. If we write the mean number of heads as $\\mu=qN$ and also assume that $q$ is small while $N$ is large, then a series of irritating limits and substitutions yields the __Poisson distribution__\n",
    "\n",
    "$P(n|\\mu) = \\frac{\\mu^n e^{-\\mu}}{n!}$\n",
    "\n",
    "\n",
    "This is an extremely important result, given that most astronomy and physics experiments boil down to counting events that are rare compared with the number of time intervals in which they might happen (and be recorded).\n",
    "* E.g., most obviously, the number of photons from some source hitting a particular CCD pixel during an observation.\n",
    "\n",
    "The Poisson distribution has the following (probably familiar) properties:\n",
    "* Expectation value (mean) $\\langle n\\rangle = \\mu$\n",
    "* Variance $\\left\\langle \\left(n-\\langle n \\rangle\\right)^2 \\right\\rangle = \\mu$\n",
    "* Additivity: $n_1+n_2\\sim \\mathrm{Pois}(\\mu_1+\\mu_2)$ if $n_i\\sim\\mathrm{Pois}(\\mu_i)$\n",
    "\n",
    "### The central limit theorem\n",
    "\n",
    "Another important theorem states, in its most common form:\n",
    "* If $X_i$ are independent and drawn from an identical PDF, with mean $\\mu$ and variance $\\sigma^2$, then the sum of $n$ $X$'s tends to the normal (Gaussian) distribution with mean $n\\,\\mu$ and variance $n\\,\\sigma^2$.\n",
    "* Alternatively, the average $\\sum_i X_i/n$ tends to normal with mean $\\mu$ and variance $\\sigma^2/n$.\n",
    "\n",
    "Among other things, this implies that a Poisson distribution with large enough $\\mu$ closely resembles a Gaussian.\n",
    "\n",
    "#### Cautions\n",
    "\n",
    "This is a powerful result, but we need to keep some things in mind.\n",
    "1. It doesn't tell us, in general, how big $n$ needs to be for things to become \"Gaussian enough\" for a given purpose. This would need to be determined by more careful analysis.\n",
    "2. It's tempting to bin up data (e.g. Poisson counts in adjacent pixels/channels/integrations) enough to justify using the simple Gaussian distribution, but this risks throwing away key information in the data set (e.g. spatial/spectral/temporal structure)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Models\n",
    "\n",
    "Goals:\n",
    "* Introduce generative models in the context of mocking data and inference\n",
    "* Introduce probabilistic graphical models as a tool for model visualization\n",
    "* Practice building some simple models\n",
    "\n",
    "## Optional further reading\n",
    "(Don't buy these for this course, these are just pointers if you'd like to learn more.)\n",
    "* Ivezic et al, 'Statistics, Data Mining, and Machine Learning in Astronomy', Sections 3.3 and 3.7\n",
    "* Bishop, 'Pattern Recognition and Machine Learning,' Sections 8.1 and 8.2\n",
    "\n",
    "A **generative model** formalizes our understanding of how a data set comes to exist, including\n",
    "* physical processes happening out there in the Universe\n",
    "* instrumental effects and the measurement process\n",
    "* any computations done prior to calling the result a \"data set\"\n",
    "\n",
    "In other words, it's what we need in order to generate a mock data set.\n",
    "\n",
    "To actually generate mock data, we need to specify the **sampling distribution**,  $p(\\mathrm{data}|\\mathrm{model})$. This PDF is the mathemetical expression of our generative model.\n",
    "\n",
    "* The assumed \"$\\mathrm{model}$\" specifies the form and parameters of the sampling distribution\n",
    "* A random draw from $P(\\mathrm{data}|\\mathrm{model})$ is a dataset, \"$\\mathrm{data}$\"\n",
    "\n",
    "<!--\n",
    "* It shows up directly in Bayes Theorem, and ideally (as a function of model parameters) as the likelihood function in maximum-likelihood fitting.\n",
    "-->\n",
    "\n",
    "What are generative models useful for?\n",
    "\n",
    "* Performing inference: constructing the *sampling distribution* or *likelihood function*\n",
    "* Testing inference: does our analysis, run on mock data, recover the input model?\n",
    "* Checking inferences: do mock data generated from a fitted model resemble the real data?\n",
    "\n",
    "A **probabilistic graphical model** (PGM) is a very useful way of visualizing a generative model.\n",
    "* They sketch out the procedure for how one would generate mock data in practice.\n",
    "* They illustrate the interdependence of model parameters, and the dependence of data on parameters.\n",
    "* _They also (therefore) represent a conditional factorization of the PDF for all the data and model parameters._\n",
    "\n",
    "**Many, many** mistakes can be avoided by sketching out a PGM at the outset of a statistical analysis.\n",
    "\n",
    "Technically, a PGM is a type of *directed acyclic graph*, where **nodes** and **edges** represent parts of the model.\n",
    "\n",
    "Let's look at a very simple example...\n",
    "\n",
    "Here's an image (and a zoom-in):\n",
    "\n",
    "<table><tr width=90%>\n",
    "<td><img src=\"graphics/tour_cluster_image.png\" height=300></td>\n",
    "<td><img src=\"graphics/tour_cluster_image_zoom.png\" height=300></td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our measurement is the number of counts in each pixel. Here is a generative model:\n",
    "* There's an object emitting light, whose properties are parametrized by $\\theta$.\n",
    "* From $\\theta$, we can determine the average flux falling on a given pixel $k$, $F_k$.\n",
    "* Given the exposure time of our observation, $T$, and some conversion factors, $F_k$ determines the average number of counts expected, $\\mu_k$.\n",
    "* The number of counts measured, $N_k$, is a Poisson draw, given the average $\\mu_k$.\n",
    "\n",
    "Notice that the model was described in terms of conditional relationships.\n",
    "* $\\theta \\Rightarrow F_k$\n",
    "* $F_k,T \\Rightarrow \\mu_k$\n",
    "* $N_k \\sim \\mathrm{Poisson}(\\mu_k)$\n",
    "\n",
    "The PGM will do the same, visually.\n",
    "\n",
    "This is what it looks like:\n",
    "<img src=\"graphics/pgms_pixelcounts.png\">\n",
    "\n",
    "Ingredients of a PGM:\n",
    "* **Nodes** represent PDFs for parameters\n",
    "* **Edges** represent conditional relationships\n",
    "* **Plates** represent repeated model components whose contents are conditionally independent\n",
    "\n",
    "Types of nodes:\n",
    "* **Circles** represent a PDF. This parameter is a *stochastic* function of the parameters feeding into it.\n",
    "* **Points** represent a delta-function PDF. This parameter is a *deterministic* function of the parameters feeding into it.\n",
    "* **Double circles** (or shading) indicate measured data. They are stochastic in the context of generating mock data, but fixed in the context of parameter inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "How are these PGMs different, and what does the difference mean?\n",
    "\n",
    "<table><tr><td>\n",
    "<img src=\"graphics/pgms_pixelcounts.png\">\n",
    "</td><td>\n",
    "<img src=\"graphics/pgms_pixelcounts2.png\">\n",
    "</td></tr></table>\n",
    "\n",
    " \n",
    " \n",
    "\n",
    "By mapping the conditional dependences of a model, PGMs illustrate how to factorize (and hence draw samples from) the joint PDF for all variables:\n",
    "\n",
    "$p(\\theta,T,\\{F_k, \\mu_k, N_k\\}) = p(\\theta)p(T) \\prod_k P(N_k|\\mu_k)p(\\mu_k|F_k,T)p(F_k|\\theta)$\n",
    "\n",
    "<img src=\"graphics/pgms_pixelcounts.png\">\n",
    "\n",
    "In this case, some PDFs are delta functions, so we can straightforwardly marginalize over such _deterministic_ variables:\n",
    "\n",
    "$p(\\theta,T,\\{\\mu_k, N_k\\}) = \\int dF_k\\; p(\\theta)p(T) \\prod_k P(N_k|\\mu_k)p(\\mu_k|F_k,T)p(F_k|\\theta)$\n",
    "\n",
    "$= \\underbrace{p(\\theta)} ~ \\underbrace{\\prod_k P\\left(N_k|\\mu_k(\\theta,T)\\right)}$\n",
    "$= \\mathrm{prior}(\\theta) ~\\times~ (\\mathrm{sampling~distribution~of~}\\vec{N})$\n",
    "\n",
    "### Exercise\n",
    "\n",
    "<table width=60%><tr>\n",
    "    <td><img src=\"graphics/pgms_a-c-d.png\"></td>\n",
    "    <td><img src=\"graphics/pgms_c-y-d.png\"></td>\n",
    "</tr></table>\n",
    "\n",
    "* On your own, write down the probability expressions illustrated by these two graphs. \n",
    "* When you're ready, raise your hand (in zoom) to report back to the class.\n",
    "\n",
    "\n",
    "### Take-home messages\n",
    "\n",
    "* Both simulation of mock data and model inference from data require a model for how the Universe (or our computer) generates data.\n",
    "* PGMs are a helpful way of visualizing the conditional dependences of a model (how the probability expressions factorize).\n",
    "\n",
    "Note: the `daft` Python package is useful for making pretty PGMs.\n",
    "\n",
    "### Exercise: linear regression\n",
    "\n",
    "Your data is a list of $\\{x_k,y_k,\\sigma_k\\}$ triplets, where $\\sigma_k$ is some estimate of the \"error\" on $y_k$. You think a linear model, $y(x)=a+bx$, might explain these data. To start exploring this idea, you decide to generate some simulated data, to compare with your real dataset.\n",
    "\n",
    "In the absence of any better information, assume that $\\vec{x}$ and $\\vec{\\sigma}$ are (somehow) known precisely, and that the \"error\" on $y_k$ is Gaussian (mean of $a+bx_k$ and standard deviation $\\sigma_k$).\n",
    "\n",
    "1. Draw the PGM, and write down the corresponding probability expressions, for this problem.\n",
    "\n",
    "2. What (unspecified) assumptions, if any, would you have to make to actually generate data? Which assumptions do you think are unlikely to hold in practice? Choose one (or more) of these assumptions and work out how to generalize the PGM/generative model to avoid making it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
